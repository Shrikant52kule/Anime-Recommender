# -*- coding: utf-8 -*-
"""Anime Recommender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15awqMD3TNpzyOYZppkovz7I6ydOisubS
"""

from google.colab import drive
drive.mount('/content/drive/')

import pandas as pd
# Define file paths (Update these paths based on your file location)
anime_path = "/content/drive/MyDrive/Anime Recommendation/anime.csv"
ratings_path = "/content/drive/MyDrive/Anime Recommendation/rating.csv"

# Load datasets
anime_df = pd.read_csv(anime_path)
ratings_df = pd.read_csv(ratings_path)

# Display first few rows
print("Anime Dataset:")
print(anime_df.head())

print("\nRatings Dataset:")
print(ratings_df.head())

# Check missing values
print("\nMissing values in anime dataset:")
print(anime_df.isnull().sum())

print("\nMissing values in ratings dataset:")
print(ratings_df.isnull().sum())

# Drop rows with missing values (if necessary)
anime_df.dropna(inplace=True)
ratings_df.dropna(inplace=True)

# Convert 'genre' column into a list of genres
anime_df['genre'] = anime_df['genre'].apply(lambda x: x.split(', ') if isinstance(x, str) else [])

# Display processed genres
print("\nProcessed Genres:")
print(anime_df[['name', 'genre']].head())

# Check dataset shapes
print(f"Anime Dataset Shape: {anime_df.shape}")
print(f"Ratings Dataset Shape: {ratings_df.shape}")

# Summary statistics
print("\nAnime Dataset Info:")
print(anime_df.info())

print("\nRatings Dataset Info:")
print(ratings_df.info())

import matplotlib.pyplot as plt
import seaborn as sns

# Plot rating distribution
plt.figure(figsize=(8, 5))
sns.histplot(ratings_df['rating'], bins=10, kde=True, color='blue')
plt.title("Distribution of Ratings")
plt.xlabel("Rating")
plt.ylabel("Count")
plt.show()

# Count the number of ratings per anime
anime_ratings_count = ratings_df.groupby('anime_id')['rating'].count().reset_index()

# Merge with anime details
anime_ratings_count = anime_ratings_count.merge(anime_df[['anime_id', 'name']], on='anime_id')

# Sort by most rated
most_rated_anime = anime_ratings_count.sort_values(by='rating', ascending=False)

# Display top 10 most rated anime
print("\nTop 10 Most Rated Anime:")
print(most_rated_anime.head(10))

from collections import Counter

# Flatten all genres into a single list
all_genres = [genre for sublist in anime_df['genre'] for genre in sublist]

# Count occurrences
genre_counts = Counter(all_genres)

# Convert to DataFrame
genre_df = pd.DataFrame(genre_counts.items(), columns=['Genre', 'Count']).sort_values(by='Count', ascending=False)

# Plot the most common genres
plt.figure(figsize=(10, 5))
sns.barplot(x=genre_df['Genre'][:10], y=genre_df['Count'][:10], palette="viridis")
plt.xticks(rotation=45)
plt.title("Top 10 Most Popular Anime Genres")
plt.xlabel("Genre")
plt.ylabel("Count")
plt.show()

# Calculate total unique users and anime
num_users = ratings_df['user_id'].nunique()
num_anime = ratings_df['anime_id'].nunique()

# Calculate sparsity
sparsity = (1.0 - (len(ratings_df) / (num_users * num_anime))) * 100
print(f"\nDataset Sparsity: {sparsity:.2f}%")

# Assuming you want to print the column names of the dataframe 'anime_df'
print(anime_df.columns)
print(ratings_df.columns)

# OR, if you want to print a specific column name, replace 'column_name'
# with the actual name of the column you are interested in
column_name = 'name'  # Replace with your desired column name
print(column_name)

import pandas as pd

# Step 1: Reduce dataset size (Keep only top-rated anime & active users)
top_anime = ratings_df['anime_id'].value_counts().head(2000).index  # Keep top 2000 most-rated anime
filtered_ratings_df = ratings_df[ratings_df['anime_id'].isin(top_anime)]

user_counts = filtered_ratings_df['user_id'].value_counts()
active_users = user_counts[user_counts >= 10].index  # Keep users who rated at least 10 anime
filtered_ratings_df = filtered_ratings_df[filtered_ratings_df['user_id'].isin(active_users)]

# Step 2: Aggregate duplicate ratings
filtered_ratings_df = filtered_ratings_df.groupby(['user_id', 'anime_id'])['rating'].mean().reset_index()

# Step 3: Convert to Sparse Matrix to Prevent Crashes
from scipy.sparse import csr_matrix

# Pivot ratings data into a sparse matrix
anime_ratings_pivot = filtered_ratings_df.pivot(index='user_id', columns='anime_id', values='rating').fillna(0)
anime_ratings_sparse = csr_matrix(anime_ratings_pivot.values)

print(f"Shape of ratings matrix: {anime_ratings_pivot.shape}")

# Check if anime_ratings_pivot contains any columns (anime IDs)
if anime_ratings_pivot.shape[1] == 0:
    print("Error: No anime IDs found after filtering!")
else:
    print(f"Total anime IDs available: {anime_ratings_pivot.shape[1]}")
    print("Some available anime IDs:", list(anime_ratings_pivot.columns)[:10])  # Show first 10 IDs

top_anime = ratings_df['anime_id'].value_counts().head(5000).index  # Keep top 5000 most-rated anime

from sklearn.neighbors import NearestNeighbors
from scipy.sparse import csr_matrix

# Rebuild the sparse matrix to match KNN training dimensions
anime_ratings_pivot = anime_ratings_pivot.loc[:, list(anime_ratings_pivot.columns)[:2000]]  # Keep only 2000 anime
anime_ratings_sparse = csr_matrix(anime_ratings_pivot.values)

# Train KNN Model
knn = NearestNeighbors(metric='cosine', algorithm='brute')
knn.fit(anime_ratings_sparse.T)  # Transpose to train correctly

print("KNN Model trained successfully on", anime_ratings_pivot.shape)

def get_similar_anime(anime_id, n=5):
    try:
        if anime_id not in anime_ratings_pivot.columns:
            return f"Anime ID {anime_id} not found in the dataset!"

        anime_idx = list(anime_ratings_pivot.columns).index(anime_id)  # Find index of anime_id
        distances, indices = knn.kneighbors(anime_ratings_sparse.T[anime_idx].reshape(1, -1), n_neighbors=n+1)

        # Get similar anime IDs (excluding itself)
        similar_anime_ids = [anime_ratings_pivot.columns[i] for i in indices.flatten()[1:]]

        # Fetch names from anime_df
        return anime_df[anime_df['anime_id'].isin(similar_anime_ids)][['anime_id', 'name']]

    except Exception as e:
        return f"Error: {str(e)}"

# Example usage
sample_anime_id = list(anime_ratings_pivot.columns)[0]  # Select first valid anime ID
similar_anime = get_similar_anime(sample_anime_id, 5)
print(similar_anime)

# Keep users who have rated at least 50 anime
user_counts = ratings_df['user_id'].value_counts()
active_users = user_counts[user_counts >= 50].index
filtered_ratings_df = ratings_df[ratings_df['user_id'].isin(active_users)]

# Keep only anime that have at least 100 ratings
anime_counts = filtered_ratings_df['anime_id'].value_counts()
popular_anime = anime_counts[anime_counts >= 100].index
filtered_ratings_df = filtered_ratings_df[filtered_ratings_df['anime_id'].isin(popular_anime)]

print("Reduced Ratings Dataset Shape:", filtered_ratings_df.shape)

from sklearn.neighbors import NearestNeighbors
from scipy.sparse import csr_matrix

# Aggregate duplicate ratings before pivoting
filtered_ratings_df = filtered_ratings_df.groupby(['user_id', 'anime_id'])['rating'].mean().reset_index()

# Pivot to create User-Item Matrix
user_ratings_pivot = filtered_ratings_df.pivot(index='user_id', columns='anime_id', values='rating').fillna(0)

# Check the new shape
print("Final User-Rating Matrix Shape:", user_ratings_pivot.shape)

from scipy.sparse import csr_matrix

# Convert the pivot table to a sparse matrix
user_ratings_sparse = csr_matrix(user_ratings_pivot.values)

# Check the new shape
print("Sparse User-Ratings Matrix Shape:", user_ratings_sparse.shape)

from sklearn.neighbors import NearestNeighbors

# Initialize KNN model
user_knn = NearestNeighbors(metric='cosine', algorithm='brute')

# Train the model on the sparse user ratings matrix
user_knn.fit(user_ratings_sparse)

print("âœ… User-Based KNN Model Trained Successfully!")

def get_user_based_recommendations(user_id, n=5):
    if user_id not in user_ratings_pivot.index:
        return "User ID not found in dataset!"

    # Get the index of the user in the matrix
    user_idx = list(user_ratings_pivot.index).index(user_id)

    # Find nearest neighbors (similar users)
    distances, indices = user_knn.kneighbors(user_ratings_sparse[user_idx], n_neighbors=n+1)

    # Get similar users (excluding the user itself)
    similar_users = [user_ratings_pivot.index[i] for i in indices.flatten()[1:]]

    # Get anime watched by similar users
    recommended_anime = ratings_df[ratings_df['user_id'].isin(similar_users)] \
                        .groupby('anime_id')['rating'].mean() \
                        .sort_values(ascending=False) \
                        .head(n)

    # Fetch anime names
    return anime_df[anime_df['anime_id'].isin(recommended_anime.index)][['anime_id', 'name']]

# Example: Get recommendations for a user
sample_user_id = user_ratings_pivot.index[0]  # Pick the first user
recommended_anime = get_user_based_recommendations(sample_user_id, 5)
print(recommended_anime)

print(anime_df.columns)

import pandas as pd

# ... (your existing code) ...

# Fill missing values with an empty string
anime_df['genre'] = anime_df['genre'].fillna('')
anime_df['type'] = anime_df['type'].fillna('')
anime_df['rating'] = anime_df['rating'].fillna(0).astype(str)  # Convert rating to string
anime_df['members'] = anime_df['members'].fillna(0).astype(str)  # Convert members to string

# Convert 'genre' column from list to string (using ', ' as separator)
anime_df['genre'] = anime_df['genre'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)

# Combine relevant features into a single text column
anime_df['combined_features'] = anime_df['genre'] + " " + anime_df['type'] + " Rating:" + anime_df['rating'] + " Members:" + anime_df['members']

# Display a sample
print(anime_df[['anime_id', 'name', 'combined_features']].head())

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TF-IDF Vectorizer
tfidf = TfidfVectorizer(stop_words='english')

# Transform the combined text features into TF-IDF vectors
tfidf_matrix = tfidf.fit_transform(anime_df['combined_features'])

# Check matrix shape
print("TF-IDF Matrix Shape:", tfidf_matrix.shape)

from sklearn.metrics.pairwise import cosine_similarity

# Compute cosine similarity matrix
cosine_sim = cosine_similarity(tfidf_matrix)

# Check shape
print("Cosine Similarity Matrix Shape:", cosine_sim.shape)

def get_similar_anime(anime_name, n=5):
    # Find the anime index
    idx = anime_df[anime_df['name'] == anime_name].index

    if len(idx) == 0:
        return "Anime not found!"

    idx = idx[0]  # Extract index

    # Get similarity scores for all anime
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort by similarity score
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get top N similar anime (excluding itself)
    top_anime_indices = [i[0] for i in sim_scores[1:n+1]]

    # Return recommended anime
    return anime_df.iloc[top_anime_indices][['anime_id', 'name']]

# Example: Get recommendations for "Sword Art Online"
recommended_anime = get_similar_anime("Sword Art Online", 5)
print(recommended_anime)

from sklearn.metrics import mean_squared_error
import numpy as np

# Sample actual and predicted ratings (Replace with real values)
actual_ratings = [4, 5, 3, 4, 5]
predicted_ratings = [3.8, 4.6, 3.2, 4.1, 4.9]

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))
print(f"RMSE: {rmse}")

def precision_at_k(actual, predicted, k):
    relevant_items = set(actual[:k])  # Get top-K actual anime
    recommended_items = set(predicted[:k])  # Get top-K recommended anime
    return len(relevant_items & recommended_items) / k

# Example lists (Replace with actual values)
actual_top_k = [11061, 11757, 11617, 14345, 125]
predicted_top_k = [11061, 11757, 11617, 20237, 29429]

precision_k = precision_at_k(actual_top_k, predicted_top_k, k=5)
print(f"Precision@5: {precision_k}")

def hit_ratio(actual, predicted):
    hits = sum(1 for item in predicted if item in actual)
    return hits / len(actual)

# Example watch history & recommendations (Replace with real data)
user_watch_history = [11061, 11757, 11617, 14345, 125]
recommended_anime = [11061, 11757, 29429, 32136, 25815]

hit_ratio_score = hit_ratio(user_watch_history, recommended_anime)
print(f"Hit Ratio: {hit_ratio_score}")

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pickle

# Load anime dataset
anime_df = pd.read_csv("/content/drive/MyDrive/Anime Recommendation/anime.csv")

# Ensure 'combined_features' column exists
if 'combined_features' not in anime_df.columns:
    anime_df['combined_features'] = anime_df[['genre', 'type', 'rating', 'members']].astype(str).agg(' '.join, axis=1)

# Initialize TF-IDF Vectorizer
tfidf = TfidfVectorizer(stop_words='english')

# Transform the combined text features into TF-IDF vectors
tfidf_matrix = tfidf.fit_transform(anime_df['combined_features'])

# Compute cosine similarity matrix
cosine_similarity_matrix = cosine_similarity(tfidf_matrix)

# Save cosine similarity matrix
with open("cosine_similarity.pkl", "wb") as file:
    pickle.dump(cosine_similarity_matrix, file)

print("Cosine Similarity Matrix Generated & Saved Successfully!")

print(anime_df[anime_df['name'] == "Sword Art Online"])

def get_anime_recommendations(anime_name, df=anime_df):
    if anime_name not in df['name'].values:
        return "Anime not found!"

    idx = df[df['name'] == anime_name].index[0]

    sim_scores = list(enumerate(cosine_similarity_matrix[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:6]

    print("Similarity Scores:", sim_scores)  # Debugging step

    recommendations = [df.iloc[i[0]]['name'] for i in sim_scores]
    return recommendations

print(get_anime_recommendations("Sword Art Online"))

print(tfidf_matrix.shape)

anime_df['combined_features'] = anime_df[['genre', 'type', 'rating']].astype(str).agg(' '.join, axis=1)

# Reinitialize vectorizer
tfidf = TfidfVectorizer(stop_words='english')

# Transform text features into TF-IDF vectors
tfidf_matrix = tfidf.fit_transform(anime_df['combined_features'])

# Compute cosine similarity matrix again
cosine_similarity_matrix = cosine_similarity(tfidf_matrix)

# Save it again
import pickle
with open("cosine_similarity.pkl", "wb") as file:
    pickle.dump(cosine_similarity_matrix, file)

print("New Cosine Similarity Matrix Generated!")

def get_anime_recommendations(anime_name, df=anime_df, min_members=10000):
    if anime_name not in df['name'].values:
        return "Anime not found!"

    idx = df[df['name'] == anime_name].index[0]
    sim_scores = list(enumerate(cosine_similarity_matrix[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:11]  # Get top 10

    # Filter out unpopular anime
    recommendations = [df.iloc[i[0]]['name'] for i in sim_scores if df.iloc[i[0]]['members'] >= min_members][:5]

    return recommendations

print(get_anime_recommendations("Sword Art Online"))

anime_df['weighted_features'] = anime_df.apply(lambda x: f"{x['genre']} {x['type']} Rating:{x['rating']**2} Members:{x['members']}", axis=1)

tfidf_matrix = tfidf.fit_transform(anime_df['weighted_features'])
cosine_similarity_matrix = cosine_similarity(tfidf_matrix)

from sklearn.metrics import jaccard_score
from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()
anime_df['genre'] = anime_df['genre'].fillna('')  # Replace NaN with empty string
genre_matrix = mlb.fit_transform(anime_df['genre'].astype(str).str.split(', '))


jaccard_sim = cosine_similarity(genre_matrix)

def get_diverse_recommendations(anime_name):
    idx = anime_df[anime_df['name'] == anime_name].index[0]
    sim_scores = list(enumerate(jaccard_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:6]
    return [anime_df.iloc[i[0]]['name'] for i in sim_scores]

print(get_diverse_recommendations("Sword Art Online"))

import numpy as np
import pandas as pd
import pickle

# Save Cosine Similarity Matrix
np.save("cosine_similarity_matrix.npy", cosine_similarity_matrix)

# Save Anime DataFrame
anime_df.to_csv("anime_data.csv", index=False)

# Save the TF-IDF Model
with open("tfidf_vectorizer.pkl", "wb") as f:
    pickle.dump(tfidf, f)

print("âœ… Model and data saved successfully!")

import requests

hf_url = "https://your-space-name.hf.space"  # Replace with your actual URL

response = requests.get(f"{hf_url}/recommend", params={"anime_name": "Naruto"})

if response.status_code == 200:
    print("Response:", response.json())